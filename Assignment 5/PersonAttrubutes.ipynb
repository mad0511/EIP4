{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of PersonAttrubutes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mad0511/EIP4/blob/master/Assignment%205/PersonAttrubutes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9gm3Uani-54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q \"/content/gdrive/My Drive/person_dataset.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OCgGu9Uh6R_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "annotation_json = Path(\"via_export_json.json\")\n",
        "images_root = Path(\"imagesa\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfVosaK5iS68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "ann_list = [\n",
        "    dict({\n",
        "        \"file_name\": str(images_root/file_name).split(\".jpg\")[0]+\".jpg\"}, \n",
        "         **ann[\"regions\"][0][\"region_attributes\"]\n",
        "    ) for file_name, ann in tqdm(json.loads(annotation_json.read_text()).items())\n",
        "]\n",
        "df = pd.DataFrame(ann_list[3:]) #remove [3:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRUbVETGia_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38WJyDm3JnRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.describe().T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku0nb4MasDVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gender_encoder = LabelEncoder().fit(df.Gender)\n",
        "print(gender_encoder.classes_)\n",
        "gender_encoder.transform(df.Gender)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEEqKK4bGAXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_multi_categories(x, n=3, order_dict=None):\n",
        "    \"\"\"`x` is numpy array\"\"\"\n",
        "    if order_dict is None:\n",
        "        x_encoded = LabelEncoder().fit_transform(x)\n",
        "        return np.eye(n)[x_encoded]\n",
        "    return np.eye(n)[order_dict[x]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6uLCwCnxr4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize_and_pad(image, size=224, fill=0):\n",
        "    orig_size = image.shape[:2] \n",
        "    ratio = float(size)/max(orig_size)\n",
        "    new_size = tuple([int(x*ratio) for x in orig_size])\n",
        "\n",
        "    image = cv2.resize(image, (new_size[1], new_size[0]))\n",
        "\n",
        "    delta_w = size - new_size[1]\n",
        "    delta_h = size - new_size[0]\n",
        "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
        "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
        "\n",
        "    color = [fill]*3\n",
        "    return cv2.copyMakeBorder(\n",
        "        image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAeJPaWz4R1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_image(filename, resize=None, augment_fn=None):\n",
        "    image = cv2.imread(filename)\n",
        "    if resize is not None:\n",
        "        image = resize_and_pad(image, resize)\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_kfa6QB4hsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = read_image(df.file_name[10], resize=224)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpUqDqcT46F9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.iloc[10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYDV8-y05Iif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv2_imshow(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRmb0KEK6V58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.file_name.apply(partial(read_image, resize=224)).values\n",
        "X = np.stack(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7FHbX6t7GjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# gender\n",
        "gender_dict = {\"female\": 0, \"male\": 1}\n",
        "y_gender = np.stack(\n",
        "    df.Gender.apply(partial(encode_multi_categories, n=2, order_dict=gender_dict)).values\n",
        ")\n",
        "\n",
        "# image quality\n",
        "image_encode_dict = dict(zip((\"Bad\", \"Average\", \"Good\"), range(3)))\n",
        "y_image_quality = np.stack(\n",
        "    df.ImageQuality.apply(partial(encode_multi_categories, n=3, order_dict=image_encode_dict)).values\n",
        ")\n",
        "\n",
        "# age\n",
        "unique_ages = df.Age.unique()\n",
        "age_dict = dict(zip(sorted(unique_ages),range(len(unique_ages))))\n",
        "y_age = np.stack(\n",
        "    df.Age.apply(partial(encode_multi_categories, n=len(unique_ages), order_dict=age_dict)).values\n",
        ")\n",
        "\n",
        "# weight\n",
        "unique_weight = ['underweight','normal-healthy', 'slightly-overweight', 'over-weight']\n",
        "weight_dict = dict(zip(unique_weight, range(len(unique_weight))))\n",
        "y_weight = np.stack(\n",
        "    df.Weight.apply(partial(encode_multi_categories, n=len(unique_weight), order_dict=weight_dict)).values\n",
        ")\n",
        "\n",
        "\n",
        "# bag\n",
        "unique_bags = df.CarryingBag.unique()\n",
        "bag_dict = dict(zip(sorted(unique_bags),range(len(unique_bags))))\n",
        "y_bag = np.stack(\n",
        "    df.CarryingBag.apply(partial(encode_multi_categories, n=len(unique_bags), order_dict=bag_dict)).values\n",
        ")\n",
        "\n",
        "# pose\n",
        "unique_poses = df.BodyPose.unique()\n",
        "pose_dict = dict(zip(sorted(unique_poses),range(len(unique_poses))))\n",
        "y_pose = np.stack(\n",
        "    df.BodyPose.apply(partial(encode_multi_categories, n=len(unique_poses), order_dict=pose_dict)).values\n",
        ")\n",
        "\n",
        "# footwear\n",
        "unique_footwears = df.Footwear.unique()\n",
        "footwear_dict = dict(zip(sorted(unique_footwears),range(len(unique_footwears))))\n",
        "y_footwear = np.stack(\n",
        "    df.Footwear.apply(partial(encode_multi_categories, n=len(unique_footwears), order_dict=footwear_dict)).values\n",
        ")\n",
        "\n",
        "# emotion\n",
        "unique_emotions = df.Emotion.unique()\n",
        "emotion_dict = dict(zip(unique_emotions, range(len(unique_emotions))))\n",
        "y_emotion = np.stack(\n",
        "    df.Emotion.apply(partial(encode_multi_categories, n=len(unique_emotions), order_dict=emotion_dict)).values\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWU_9vSO7I5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_valid, y_train_idx, y_valid_idx = train_test_split(X, range(len(X)))\n",
        "\n",
        "y_train = {\n",
        "    \"gender_output\": y_gender[y_train_idx], \n",
        "    \"image_quality_output\": y_image_quality[y_train_idx],\n",
        "    \"age_output\": y_age[y_train_idx],\n",
        "    \"weight_output\": y_weight[y_train_idx],\n",
        "    \"bag_output\": y_bag[y_train_idx],\n",
        "    \"pose_output\": y_pose[y_train_idx],\n",
        "    \"footwear_output\": y_footwear[y_train_idx],\n",
        "    \"emotion_output\": y_emotion[y_train_idx],\n",
        "\n",
        "}\n",
        "\n",
        "y_valid = {\n",
        "    \"gender_output\": y_gender[y_valid_idx], \n",
        "    \"image_quality_output\": y_image_quality[y_valid_idx],\n",
        "    \"age_output\": y_age[y_valid_idx],\n",
        "    \"weight_output\": y_weight[y_valid_idx],\n",
        "    \"bag_output\": y_bag[y_valid_idx],\n",
        "    \"pose_output\": y_pose[y_valid_idx],\n",
        "    \"footwear_output\": y_footwear[y_valid_idx],\n",
        "    \"emotion_output\": y_emotion[y_valid_idx],\n",
        "\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backbone = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "neck = backbone.output\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.5)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "# heads\n",
        "gender = Dense(2, activation=\"sigmoid\", name=\"gender_output\")(build_tower(neck))\n",
        "image_quality = Dense(3, activation=\"softmax\", name=\"image_quality_output\")(build_tower(neck))\n",
        "age = Dense(len(unique_ages), activation=\"softmax\", name=\"age_output\")(build_tower(neck))\n",
        "weight = Dense(len(unique_weight), activation=\"softmax\", name=\"weight_output\")(build_tower(neck))\n",
        "bag = Dense(len(unique_bags), activation=\"softmax\", name=\"bag_output\")(build_tower(neck))\n",
        "footwear = Dense(2, activation=\"sigmoid\", name=\"footwear_output\")(build_tower(neck))\n",
        "emotion = Dense(len(unique_emotions), activation=\"sigmoid\", name=\"emotion_output\")(build_tower(neck))\n",
        "pose = Dense(2, activation=\"sigmoid\", name=\"pose_output\")(build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJDR5bjyRYrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxWVxcbi_y6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freeze backbone\n",
        "for layer in backbone.layers:\n",
        "\tlayer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# losses = {\n",
        "# \t\"gender_output\": \"binary_crossentropy\",\n",
        "# \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "# \t\"age_output\": \"categorical_crossentropy\",\n",
        "# \t\"weight_output\": \"categorical_crossentropy\",\n",
        "\n",
        "# }\n",
        "# loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    # loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2ZRIQ7BW-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, epochs=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = model.evaluate(X_valid, y_valid, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4qcYLMO19Kv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}